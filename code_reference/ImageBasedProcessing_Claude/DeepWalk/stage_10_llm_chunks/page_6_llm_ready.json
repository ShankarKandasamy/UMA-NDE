{
  "page_number": 6,
  "total_buckets": 11,
  "last_bucket_previous_page": {
    "id": 19,
    "left_edge": 51,
    "top_edge": 52,
    "right_edge": 91,
    "bottom_edge": 91,
    "position": "right",
    "width_category": "medium",
    "y_group_id": 5,
    "confidence_avg": 0.9,
    "char_count": 1264,
    "word_count": 205,
    "texts": [
      "also still use Huffman coding to decrease frequent element",
      "access times.",
      "4.4.2 Non-random walks",
      "Some graphs are created as a by-product of agents inter-",
      "acting with a sequence of elements (e.g: users' navigation",
      "of pages on website)_ a When a graph is created by such a",
      "stream of non-random walks, we can use this process to feed",
      "the modeling phase directly: Graphs sampled in this way will",
      "not only capture information related to network structure,",
      "but also to the frequency at which paths are traversed.",
      "In our view this variant also encompasses language mod-",
      "eling: Sentences can be viewed as purposed walks through",
      "an appropriately designed language network, and language",
      "models like SkipGram are designed to capture this behavior.",
      "This approach can be combined with the streaming vari-",
      "ant (Section 4.4.1) to train features on continually evolv- a",
      "ing network without ever explicitly constructing the entire",
      "graph. Maintaining representations with this technique could",
      "enable web-scale classification without the hassles of dealing",
      "with a web-scale graph.",
      "5_ EXPERIMENTAL DESIGN",
      "In this section we provide an overview of the datasets and",
      "methods which we will use in our experiments. Code and",
      "data to reproduce our results will be available at the first",
      "author s website.",
      "5.1 Datasets"
    ]
  },
  "buckets": [
    {
      "id": 0,
      "left_edge": 12,
      "top_edge": 7,
      "right_edge": 45,
      "bottom_edge": 14,
      "position": "left",
      "width_category": "narrow",
      "y_group_id": 0,
      "confidence_avg": 0.83,
      "char_count": 89,
      "word_count": 14,
      "texts": [
        "Name BLOGCATALOG FLICKR YoUTUBE",
        "IVI",
        "[EL",
        "Iv|",
        "Labels",
        "80,513",
        "5,899,882 2,990,443",
        "195",
        "Groups",
        "1,138,499"
      ]
    },
    {
      "id": 1,
      "left_edge": 39,
      "top_edge": 11,
      "right_edge": 44,
      "bottom_edge": 14,
      "position": "center",
      "width_category": "narrow",
      "y_group_id": 0,
      "confidence_avg": 1.0,
      "char_count": 8,
      "word_count": 2,
      "texts": [
        "47",
        "Groups"
      ]
    },
    {
      "id": 2,
      "left_edge": 15,
      "top_edge": 15,
      "right_edge": 42,
      "bottom_edge": 16,
      "position": "left",
      "width_category": "narrow",
      "y_group_id": 1,
      "confidence_avg": 1.0,
      "char_count": 40,
      "word_count": 7,
      "texts": [
        "Table 1: Graphs used in our experiments."
      ]
    },
    {
      "id": 3,
      "left_edge": 51,
      "top_edge": 7,
      "right_edge": 92,
      "bottom_edge": 49,
      "position": "right",
      "width_category": "medium",
      "y_group_id": 0,
      "confidence_avg": 0.9,
      "char_count": 1614,
      "word_count": 263,
      "texts": [
        "6.1 Multi-Label Classification",
        "To facilitate the comparison between our method and the",
        "relevant baselines, we use the exact same datasets and exper-",
        "imental procedure as in [39,40]. Specifically; we randomly",
        "sample a portion (TR) of the labeled nodes, and use them",
        "as training data. The rest of the nodes are used as test. We",
        "repeat this process 10 times; and report the average per -",
        "formance in terms of both Macro-Fi and Micro-Fi. When",
        "possible we report the original results [39,40] here directly:",
        "For all models we use a one- VS-rest logistic regression im-",
        "plemented by LibLinear [10] for classification. We present",
        "results for DEEPWALK with (y 80, W 10, d = 128) . The",
        "results for (SpectralClustering;  Modularity, EdgeCluster)",
        "use Tang and Liu's preferred dimensionality; d 500. =",
        "6.1.1 BlogCatalog",
        "In this experiment we increase the training ratio (TR) on",
        "the BLOGCATALOG network from 10% to 90%. Our results",
        "are presented in Table 2. Numbers in bold represent the",
        "highest performance in each column.",
        "DEEPWALK performs consistently better than EdgeCluster,",
        "Modularity, and wvRN. In fact, when trained with only 20%",
        "of the nodes labeled; DEEPWALK performs better than these",
        "approaches when they are given 90% ofthe data. The perfor-",
        "mance of SpectralClustering proves much more competitive,",
        "but DEEPWALK still outperforms when labeled data is sparse",
        "on both Macro-F1 (TR < 209) and Micro-F1 (TR < 60%).",
        "This strong performance when only small fractions of the",
        "graph are labeled is a core strength of our approach: In the",
        "following experiments, we investigate the performance of our",
        "representations on even more sparsely labeled graphs."
      ]
    },
    {
      "id": 4,
      "left_edge": 54,
      "top_edge": 50,
      "right_edge": 62,
      "bottom_edge": 50,
      "position": "center",
      "width_category": "narrow",
      "y_group_id": 2,
      "confidence_avg": 0.4,
      "char_count": 10,
      "word_count": 4,
      "texts": [
        "1 O T71: 1"
      ]
    },
    {
      "id": 5,
      "left_edge": 51,
      "top_edge": 51,
      "right_edge": 91,
      "bottom_edge": 91,
      "position": "right",
      "width_category": "medium",
      "y_group_id": 2,
      "confidence_avg": 0.91,
      "char_count": 1540,
      "word_count": 254,
      "texts": [
        "In this experiment we vary the training ratio (TR) on the",
        "FLICKR network from 1% to 10%. This corresponds to hav-",
        "ing approximately 800 to 8,000 nodes   labeled for classifi-",
        "cation in the entire network: Table 3 presents our results",
        "which are consistent with the previous experiment. DEEP -",
        "WALK outperforms all baselines by at least 3% with respect",
        "to Micro-F1. Additionally, its Micro-F1 performance when",
        "only 3% of the graph is   labeled beats   all other methods",
        "even when they have been given 10% of the data: In other",
        "words, DEEPWALK can outperform the baselines with 60%",
        "less training data: It also performs quite well in Macro-Fl,",
        "initially performing close to SpectralClustering; but distanc-",
        "ing itself to 1% improvement. a",
        "6.1.3 YouTube",
        "The YoUTUBE  network is considerably larger than the",
        "previous ones we have experimented on, and its size pre-",
        "vents two of our baseline methods (SpectralClustering and",
        "Modularity) from running on it. It is much closer to a real",
        "world graph than those we have previously considered:",
        "The results of varying the training ratio (TR) from 1% to",
        "10% are presented in Table 4. They show that DEEPWALK",
        "significantly outperforms the scalable baseline for creating",
        "graph representations; EdgeCluster. When 1% of the la-",
        "beled nodes are used for test, the Micro-Fi improves by",
        "14%. The Macro-Fi shows corresponding 10% increase. a",
        "This lead narrows as the training data increases, but DEEP-",
        "WALK ends with a 3% lead in Micro-Fl, and an impressive",
        "5% improvement in Macro-Fi.",
        "This experiment showcases the performance benefits that"
      ]
    },
    {
      "id": 6,
      "left_edge": 20,
      "top_edge": 8,
      "right_edge": 26,
      "bottom_edge": 13,
      "position": "left",
      "width_category": "narrow",
      "y_group_id": 0,
      "confidence_avg": 0.81,
      "char_count": 24,
      "word_count": 4,
      "texts": [
        "10,312",
        "333,983",
        "39",
        "Interests"
      ]
    },
    {
      "id": 7,
      "left_edge": 9,
      "top_edge": 20,
      "right_edge": 48,
      "bottom_edge": 79,
      "position": "left",
      "width_category": "narrow",
      "y_group_id": 1,
      "confidence_avg": 0.91,
      "char_count": 1812,
      "word_count": 295,
      "texts": [
        "An overview of the graphs we consider in our experiments",
        "is given in Figure 1.",
        "BLOGCATALOG [39] is a network of social relationships",
        "provided by blogger authors. The labels represent the",
        "topic categories provided by the authors.",
        "FLICKR [39] is a network of the contacts between users",
        "of the photo sharing website: The labels represent the",
        "interest groups of the users such as black  and white",
        "photos' _",
        "YouTUBE  [40] is social network a between users of",
        "the popular video sharing website. The labels here",
        "represent groups of viewers that enjoy common video",
        "genres (e.g: anime and wrestling).",
        "5.2 Baseline Methods",
        "To validate the performance of our approach we compare",
        "it against a number of baselines:",
        "SpectralClustering [41]: This method generates a rep-",
        "resentation in Rd from the d-smallest eigenvectors of",
        "L; the normalized graph Laplacian of G. Utilizing the",
        "f 0 LJ",
        "will be useful for classification:",
        "Modularity [39]: This method generates a representa-",
        "tion in Rd from the top-d eigenvectors of B, the Mod-",
        "ularity matrix f G. The eigenvectors of B encode",
        "information about modular graph partitions of G [34].",
        "Using them as features assumes that modular graph",
        "partitions will be useful for classification.",
        "EdgeCluster [40]: This method uses k-means cluster-",
        "ing to cluster the   adjacency matrix of G. Its has",
        "been shown to perform comparably to the Modularity",
        "method, with the added advantage of scaling to graphs",
        "which are too large for spectral decomposition:",
        "WVRN [24]: The weighted-vote Relational Neighbor is",
        "a relational classifier. Given the neighborhood Ni of",
        "vertex Ui , wvRN estimates Pr(yi|Ni) with the (appro",
        "priately normalized) weighted mean of its neighbors",
        "(i.e Pr(yi|Ni)",
        "shown surprisingly good performance in real networks,",
        "and has been advocated as a sensible relational classi-",
        "fication baseline [25].",
        "Uij Pr(yj Nj)): It has"
      ]
    },
    {
      "id": 8,
      "left_edge": 37,
      "top_edge": 50,
      "right_edge": 45,
      "bottom_edge": 51,
      "position": "center",
      "width_category": "narrow",
      "y_group_id": 2,
      "confidence_avg": 0.36,
      "char_count": 13,
      "word_count": 3,
      "texts": [
        "UILCU 51 WpII"
      ]
    },
    {
      "id": 9,
      "left_edge": 25,
      "top_edge": 74,
      "right_edge": 26,
      "bottom_edge": 75,
      "position": "left",
      "width_category": "narrow",
      "y_group_id": 3,
      "confidence_avg": 0.42,
      "char_count": 1,
      "word_count": 1,
      "texts": [
        "2"
      ]
    },
    {
      "id": 10,
      "left_edge": 9,
      "top_edge": 79,
      "right_edge": 48,
      "bottom_edge": 91,
      "position": "left",
      "width_category": "narrow",
      "y_group_id": 3,
      "confidence_avg": 0.94,
      "char_count": 295,
      "word_count": 46,
      "texts": [
        "Majority: This naive method simply chooses the most",
        "frequent labels in the training set_",
        "6_ EXPERIMENTS",
        "In this section we present an experimental analysis of our",
        "method. We thoroughly evaluate it on a number of multi-",
        "label classification tasks, and analyze its sensitivity across",
        "several parameters."
      ]
    }
  ]
}