{
  "page_number": 2,
  "total_buckets": 4,
  "last_bucket_previous_page": {
    "id": 18,
    "left_edge": 8,
    "top_edge": 87,
    "right_edge": 48,
    "bottom_edge": 91,
    "position": "left",
    "width_category": "medium",
    "y_group_id": 7,
    "confidence_avg": 0.7,
    "char_count": 215,
    "word_count": 36,
    "texts": [
      "The authors, 2014. This is the author $ draft of the work: It is posted here",
      "for your personal use. Not for redistribution: The definitive version was",
      "published in KDD' 14, http: / /dx.doi.org/10.1145/2623330 _",
      "2623732"
    ]
  },
  "buckets": [
    {
      "id": 0,
      "left_edge": 8,
      "top_edge": 7,
      "right_edge": 48,
      "bottom_edge": 53,
      "position": "left",
      "width_category": "medium",
      "y_group_id": 0,
      "confidence_avg": 0.91,
      "char_count": 1736,
      "word_count": 248,
      "texts": [
        "narios, we   evaluate its performance on challenging  multi-",
        "label network classification problems in large heterogeneous",
        "graphs. In the relational classification problem, the links be-",
        "tween feature vectors violate the traditional i..d. assump-",
        "tion. Techniques to address this problem typically use ap-",
        "proximate inference techniques [31, 35] to leverage the de-",
        "pendency information to improve classification results. We",
        "distance ourselves from these approaches by learning label-",
        "independent representations of the graph: Our representa-",
        "tion quality is not influenced by the choice of labeled ver _",
        "tices, s0 they can be shared among tasks:",
        "DEEP WALK outperforms other latent representation meth-",
        "ods for creating social dimensions [39,41], especially when",
        "labeled nodes are scarce. Strong performance with our rep-",
        "resentations is  possible with very  simple linear classifiers",
        "(e.g: logistic regression). Our representations are general,",
        "and can be   combined with any  classification method (in-",
        "cluding iterative inference methods) DEEPWALK achieves",
        "all of that while being an online algorithm that is trivially",
        "parallelizable.",
        "Our contributions are as follows:",
        "We introduce deep learning as a tool to analyze graphs,",
        "to build robust representations that are suitable for",
        "statistical modeling: DEEPWALK learns structural reg-",
        "ularities present within short random walks:",
        "We extensively evaluate our representations on multi-",
        "label classification tasks on several social networks. We",
        "show significantly increased classification performance",
        "in the presence of label sparsity, getting improvements",
        "5%-10% of Micro F1, on the sparsest problems we con-",
        "cider Tn",
        "outperform its competitors even when given 60% less",
        "training data",
        "cacoc DEFPWAI K'c renrecentations can"
      ]
    },
    {
      "id": 1,
      "left_edge": 9,
      "top_edge": 54,
      "right_edge": 48,
      "bottom_edge": 71,
      "position": "left",
      "width_category": "narrow",
      "y_group_id": 1,
      "confidence_avg": 0.91,
      "char_count": 652,
      "word_count": 107,
      "texts": [
        "We demonstrate the scalability of our algorithm by",
        "building representations of web-scale graphs, (such as",
        "YouTube) using a parallel implementation: Moreover,",
        "we describe the minimal changes necessary to build a",
        "streaming version of our approach:",
        "The rest of the paper is arranged as follows. In Sections 2",
        "and 3, we discuss the problem formulation of classification",
        "in data networks, and how it relates to our work: In Section",
        "4 we   present DEEPWALK, our approach  for Social Repre-",
        "sentation Learning: We outline ours experiments in Section",
        "5, and present their results in Section 6. We close with a",
        "discussion of related work in Section 7, and our conclusions."
      ]
    },
    {
      "id": 2,
      "left_edge": 9,
      "top_edge": 73,
      "right_edge": 48,
      "bottom_edge": 91,
      "position": "left",
      "width_category": "narrow",
      "y_group_id": 2,
      "confidence_avg": 0.92,
      "char_count": 688,
      "word_count": 129,
      "texts": [
        "2. PROBLEM DEFINITION",
        "We consider the problem of classifying members of a social",
        "network into one or more categories. More formally, let G",
        "(V,E), where V are the members of the network; and E be",
        "its edges, E = (V x V): Given partially labeled social a",
        "(V,E,X,Y), with attributes X € RlVIx s",
        "where S is the size of the feature space for each attribute",
        "vector; and Y € RlvIxl V is the set of labels:",
        "In a traditional machine learning classification setting; we",
        "aim to learn a hypothesis H that maps elements of X to the",
        "labels set V. In our case, we can utilize the significant in-",
        "formation about the dependence of the examples embedded",
        "in the structure of G to achieve superior performance.",
        "network GL"
      ]
    },
    {
      "id": 3,
      "left_edge": 51,
      "top_edge": 7,
      "right_edge": 93,
      "bottom_edge": 91,
      "position": "right",
      "width_category": "medium",
      "y_group_id": 0,
      "confidence_avg": 0.91,
      "char_count": 2907,
      "word_count": 426,
      "texts": [
        "In the literature, this is known as the relational classifi-",
        "cation (or the collective classification problem [37]). Tradi-",
        "tional approaches to relational classification pose the prob-",
        "lem an inference in as an undirected Markov network; and",
        "then use iterative approximate inference algorithms (such",
        "as the iterative   classification algorithm [31], Gibbs Sam-",
        "pling [14], or label relaxation [18]) to compute the posterior",
        "distribution of labels given the network structure",
        "We propose a different approach to capture the network",
        "topology information. Instead of mixing the label space",
        "as part of the feature space, we propose an unsupervised",
        "method which learns features that capture the graph struc-",
        "ture independent of the labels' distribution.",
        "This separation between the structural representation and",
        "the labeling task avoids cascading errors; which can occur in",
        "iterative methods [33]. Moreover, the same representation",
        "can be used for multiple classification problems concerning",
        "that network",
        "Our goal is to learn XE RlvIxd 6 where d is small num-",
        "ber of latent dimensions. These low-dimensional represen-",
        "tations are distributed; meaning each social phenomena is",
        "expressed by a subset of the dimensions and each dimension",
        "contributes to a subset of the social concepts expressed by",
        "the space.",
        "Using these structural features; we will augment the at-",
        "tributes space to help the classification decision. These fea-",
        "tures are general, and can be used with any classification",
        "algorithm (including iterative methods). However , we be-",
        "lieve that the greatest utility of these features is their easy",
        "integration with simple machine learning algorithms: They",
        "scale appropriately in real-world networks, as we will show",
        "in Section 6.",
        "3. LEARNING SOCIAL REPRESENTATIONS",
        "We seek learning social representations with the following",
        "characteristics:",
        "Adaptability Real social networks are constantly",
        "evolving; new  social relations  should not require re-",
        "peating the learning process all over again.",
        "Community aware The distance between latent",
        "dimensions   should represent metric a for evaluating",
        "social similarity between the corresponding members",
        "of the network_ This allows generalization in networks",
        "with homophily:",
        "Low dimensional When labeled data is scarce, low-",
        "dimensional models   generalize better, and speed up",
        "convergence and inference.",
        "Continuous We require latent  representations to",
        "model partial community membership in continuous",
        "space In addition to providing nuanced a view of",
        "community membership, a continuous representation",
        "has smooth decision boundaries between communities",
        "which allows more robust classification.",
        "Our method for satisfying these requirements learns repre",
        "sentation for vertices from a stream of short random walks,",
        "using optimization  techniques originally designed for lan-",
        "guage modeling: Here, we review the basics of both random",
        "walks and language modeling; and describe how their com-",
        "bination satisfies our requirements."
      ]
    }
  ]
}