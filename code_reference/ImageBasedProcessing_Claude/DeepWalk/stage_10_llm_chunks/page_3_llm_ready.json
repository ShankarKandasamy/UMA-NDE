{
  "page_number": 3,
  "total_buckets": 23,
  "last_bucket_previous_page": {
    "id": 3,
    "left_edge": 51,
    "top_edge": 7,
    "right_edge": 93,
    "bottom_edge": 91,
    "position": "right",
    "width_category": "medium",
    "y_group_id": 0,
    "confidence_avg": 0.91,
    "char_count": 2907,
    "word_count": 426,
    "texts": [
      "In the literature, this is known as the relational classifi-",
      "cation (or the collective classification problem [37]). Tradi-",
      "tional approaches to relational classification pose the prob-",
      "lem an inference in as an undirected Markov network; and",
      "then use iterative approximate inference algorithms (such",
      "as the iterative   classification algorithm [31], Gibbs Sam-",
      "pling [14], or label relaxation [18]) to compute the posterior",
      "distribution of labels given the network structure",
      "We propose a different approach to capture the network",
      "topology information. Instead of mixing the label space",
      "as part of the feature space, we propose an unsupervised",
      "method which learns features that capture the graph struc-",
      "ture independent of the labels' distribution.",
      "This separation between the structural representation and",
      "the labeling task avoids cascading errors; which can occur in",
      "iterative methods [33]. Moreover, the same representation",
      "can be used for multiple classification problems concerning",
      "that network",
      "Our goal is to learn XE RlvIxd 6 where d is small num-",
      "ber of latent dimensions. These low-dimensional represen-",
      "tations are distributed; meaning each social phenomena is",
      "expressed by a subset of the dimensions and each dimension",
      "contributes to a subset of the social concepts expressed by",
      "the space.",
      "Using these structural features; we will augment the at-",
      "tributes space to help the classification decision. These fea-",
      "tures are general, and can be used with any classification",
      "algorithm (including iterative methods). However , we be-",
      "lieve that the greatest utility of these features is their easy",
      "integration with simple machine learning algorithms: They",
      "scale appropriately in real-world networks, as we will show",
      "in Section 6.",
      "3. LEARNING SOCIAL REPRESENTATIONS",
      "We seek learning social representations with the following",
      "characteristics:",
      "Adaptability Real social networks are constantly",
      "evolving; new  social relations  should not require re-",
      "peating the learning process all over again.",
      "Community aware The distance between latent",
      "dimensions   should represent metric a for evaluating",
      "social similarity between the corresponding members",
      "of the network_ This allows generalization in networks",
      "with homophily:",
      "Low dimensional When labeled data is scarce, low-",
      "dimensional models   generalize better, and speed up",
      "convergence and inference.",
      "Continuous We require latent  representations to",
      "model partial community membership in continuous",
      "space In addition to providing nuanced a view of",
      "community membership, a continuous representation",
      "has smooth decision boundaries between communities",
      "which allows more robust classification.",
      "Our method for satisfying these requirements learns repre",
      "sentation for vertices from a stream of short random walks,",
      "using optimization  techniques originally designed for lan-",
      "guage modeling: Here, we review the basics of both random",
      "walks and language modeling; and describe how their com-",
      "bination satisfies our requirements."
    ]
  },
  "buckets": [
    {
      "id": 0,
      "left_edge": 10,
      "top_edge": 7,
      "right_edge": 28,
      "bottom_edge": 8,
      "position": "left",
      "width_category": "narrow",
      "y_group_id": 0,
      "confidence_avg": 1.0,
      "char_count": 55,
      "word_count": 8,
      "texts": [
        "Frequency of Vertex Occurrence_in Short Random Walks 10"
      ]
    },
    {
      "id": 1,
      "left_edge": 10,
      "top_edge": 10,
      "right_edge": 13,
      "bottom_edge": 16,
      "position": "left",
      "width_category": "narrow",
      "y_group_id": 0,
      "confidence_avg": 0.83,
      "char_count": 20,
      "word_count": 7,
      "texts": [
        "103",
        "\"5 102 #",
        "101",
        "109",
        "109"
      ]
    },
    {
      "id": 2,
      "left_edge": 14,
      "top_edge": 16,
      "right_edge": 15,
      "bottom_edge": 16,
      "position": "left",
      "width_category": "narrow",
      "y_group_id": 1,
      "confidence_avg": 0.97,
      "char_count": 3,
      "word_count": 1,
      "texts": [
        "101"
      ]
    },
    {
      "id": 3,
      "left_edge": 11,
      "top_edge": 16,
      "right_edge": 27,
      "bottom_edge": 19,
      "position": "left",
      "width_category": "narrow",
      "y_group_id": 1,
      "confidence_avg": 0.97,
      "char_count": 59,
      "word_count": 10,
      "texts": [
        "Vertex visitation count 102 103 10\"",
        "(a) YouTube Social Graph"
      ]
    },
    {
      "id": 4,
      "left_edge": 23,
      "top_edge": 16,
      "right_edge": 24,
      "bottom_edge": 16,
      "position": "left",
      "width_category": "narrow",
      "y_group_id": 1,
      "confidence_avg": 1.0,
      "char_count": 3,
      "word_count": 1,
      "texts": [
        "105"
      ]
    },
    {
      "id": 5,
      "left_edge": 26,
      "top_edge": 16,
      "right_edge": 27,
      "bottom_edge": 16,
      "position": "left",
      "width_category": "narrow",
      "y_group_id": 1,
      "confidence_avg": 0.98,
      "char_count": 3,
      "word_count": 1,
      "texts": [
        "106"
      ]
    },
    {
      "id": 6,
      "left_edge": 29,
      "top_edge": 7,
      "right_edge": 46,
      "bottom_edge": 19,
      "position": "center",
      "width_category": "narrow",
      "y_group_id": 0,
      "confidence_avg": 0.86,
      "char_count": 142,
      "word_count": 30,
      "texts": [
        "106 Frequency of Word Occurrence in Wikipedia",
        "105",
        "10'",
        "1 103",
        "# 102",
        "101",
        "109",
        "109 101 102 103 10' 105 106 107",
        "Word mention count",
        "(b) Wikipedia Article Text"
      ]
    },
    {
      "id": 7,
      "left_edge": 9,
      "top_edge": 20,
      "right_edge": 48,
      "bottom_edge": 24,
      "position": "left",
      "width_category": "narrow",
      "y_group_id": 2,
      "confidence_avg": 0.84,
      "char_count": 166,
      "word_count": 26,
      "texts": [
        "Figure 2: The power-law distribution of vertices appearing",
        "in short random walks (2a) follows power-law, much like a",
        "the distribution of words in natural language (2b)."
      ]
    },
    {
      "id": 8,
      "left_edge": 8,
      "top_edge": 26,
      "right_edge": 48,
      "bottom_edge": 91,
      "position": "left",
      "width_category": "medium",
      "y_group_id": 3,
      "confidence_avg": 0.9,
      "char_count": 2512,
      "word_count": 424,
      "texts": [
        "3.1 Random Walks",
        "We denote a random walk rooted at vertex Ui as Wvi  It is",
        "a stochastic process with random variables WI w2 Ui ) Ui > Wk Ui",
        "such that Wk+1 is a vertex chosen at random from the neigh- Ui",
        "bors of vertex Uk - Random walks have been used as a sim-",
        "ilarity measure for a variety of problems in content recom-",
        "mendation [11] and community detection [1] They are also",
        "the foundation of a class of output sensitive algorithms which",
        "use them to compute local community structure information",
        "in time sublinear to the size of the input graph [38].",
        "It is this connection to local structure that motivates us to",
        "use a stream of short random walks as our basic tool for ex-",
        "tracting information from a network. In addition to captur-",
        "ing community information, using random walks as the ba-",
        "sis for our algorithm gives us two other desirable properties.",
        "First, local exploration is easy to parallelize: Several random",
        "walkers (in different threads, processes, or machines) can si-",
        "multaneously explore dlferent parts Of the same graph.",
        "ondly, relying on information obtained from short random",
        "walks make it possible to accommodate small changes in the",
        "graph structure without the need for global recomputation.",
        "We can iteratively update the learned model with new ran-",
        "dom walks from the changed region in time sub-linear to the",
        "entire graph.",
        "3.2 Connection: Power laws",
        "Having chosen online random walks as our primitive for",
        "capturing graph structure, we now need suitable method a",
        "to capture this information: If the degree distribution of",
        "a connected graph follows a power law (is   scale-free) , we",
        "observe that the frequency which vertices appear in the short",
        "random walks will also follow power-law distribution. a",
        "Word frequency in natural language follows a similar dis-",
        "tribution, and techniques from language modeling account",
        "for this distributional behavior. To emphasize this similar-",
        "ity we show two different power-law distributions in Figure",
        "2_ The first comes from a series of short random walks on",
        "a scale-free graph; and the second comes from the text of",
        "100,000 articles from the English Wikipedia.",
        "A core contribution of our work is the idea that techniques",
        "which have been used to model natural language (where the",
        "symbol frequency follows a power law distribution (or Zipf '$",
        "law) ) can be re-purposed to model community structure in",
        "networks. We spend the rest of this section reviewing the",
        "growing work in language modeling, and transforming it to",
        "learn representations of vertices which satisfy OUr criteria.",
        "3.3 Language Modeling"
      ]
    },
    {
      "id": 9,
      "left_edge": 51,
      "top_edge": 7,
      "right_edge": 91,
      "bottom_edge": 11,
      "position": "right",
      "width_category": "medium",
      "y_group_id": 0,
      "confidence_avg": 0.95,
      "char_count": 150,
      "word_count": 26,
      "texts": [
        "The goal of language modeling is estimate the likelihood",
        "of a specific sequence of words appearing in a corpus. More",
        "formally; given a sequence of words"
      ]
    },
    {
      "id": 10,
      "left_edge": 51,
      "top_edge": 14,
      "right_edge": 91,
      "bottom_edge": 31,
      "position": "right",
      "width_category": "medium",
      "y_group_id": 1,
      "confidence_avg": 0.84,
      "char_count": 664,
      "word_count": 118,
      "texts": [
        "where Wi € V (V is the vocabulary) , we would like to maxi-",
        "mize the Pr(wn|wo, U1 ,",
        "pus.",
        "Recent work in representation learning has focused on us-",
        "ing probabilistic neural networks to build general representa-",
        "tions of words which extend the scope of language modeling",
        "beyond its original goals.",
        "In this work, we present a generalization of language mod-",
        "eling to explore the graph through a stream of short random",
        "walks. These walks can be thought of short sentences and",
        "phrases in a special language. The direct analog is to @S-",
        "timate the likelihood of observing   vertex Ui given all the",
        "previous vertices visited s0 far in the random walk.",
        "Wn-1) over all the training COr-"
      ]
    },
    {
      "id": 11,
      "left_edge": 64,
      "top_edge": 12,
      "right_edge": 74,
      "bottom_edge": 13,
      "position": "right",
      "width_category": "narrow",
      "y_group_id": 0,
      "confidence_avg": 1.0,
      "char_count": 16,
      "word_count": 6,
      "texts": [
        "WI n = (wo, U1 ,"
      ]
    },
    {
      "id": 12,
      "left_edge": 76,
      "top_edge": 12,
      "right_edge": 79,
      "bottom_edge": 13,
      "position": "right",
      "width_category": "narrow",
      "y_group_id": 0,
      "confidence_avg": 0.53,
      "char_count": 4,
      "word_count": 2,
      "texts": [
        "Wn )"
      ]
    },
    {
      "id": 13,
      "left_edge": 63,
      "top_edge": 33,
      "right_edge": 73,
      "bottom_edge": 35,
      "position": "right",
      "width_category": "narrow",
      "y_group_id": 4,
      "confidence_avg": 0.74,
      "char_count": 16,
      "word_count": 5,
      "texts": [
        "Pr (Ui (01, U2 ,"
      ]
    },
    {
      "id": 14,
      "left_edge": 75,
      "top_edge": 33,
      "right_edge": 80,
      "bottom_edge": 35,
      "position": "right",
      "width_category": "narrow",
      "y_group_id": 4,
      "confidence_avg": 0.82,
      "char_count": 8,
      "word_count": 2,
      "texts": [
        ", Vi-1))"
      ]
    },
    {
      "id": 15,
      "left_edge": 52,
      "top_edge": 35,
      "right_edge": 91,
      "bottom_edge": 46,
      "position": "right",
      "width_category": "narrow",
      "y_group_id": 4,
      "confidence_avg": 0.96,
      "char_count": 416,
      "word_count": 76,
      "texts": [
        "Our goal is to learn a latent representation; not only a",
        "probability distribution of node co-occurrences_ and SO we",
        "introduce a mapping function $: v € V + Rlvixa This",
        "mapping $ represents the latent social representation aSSo-",
        "ciated with each vertex U in the graph. (In practice, wC",
        "represent $ by a |Vl X d matrix of free parameters; which",
        "will serve later on as our XE.) The problem then; is to",
        "estimate the likelihood:"
      ]
    },
    {
      "id": 16,
      "left_edge": 51,
      "top_edge": 48,
      "right_edge": 91,
      "bottom_edge": 68,
      "position": "right",
      "width_category": "medium",
      "y_group_id": 5,
      "confidence_avg": 0.91,
      "char_count": 687,
      "word_count": 116,
      "texts": [
        "(1)",
        "However as the walk length grows, computing this objec-",
        "tive function becomes unfeasible.",
        "A recent relaxation in language  modeling [26, 27] turns",
        "the prediction problem on its head. First, instead of using",
        "the context to predict missing word, it a uses one word to",
        "predict the context. Secondly; the context is composed of",
        "the words appearing to right side of the given word as well",
        "as the left side. Finally; it removes the ordering constraint",
        "on the problem: Instead, the model is required to maximize",
        "the probability of any word appearing in the context without",
        "the knowledge of its offset from the given word:",
        "In terms of vertex representation modeling; this yields the",
        "optimization problem:"
      ]
    },
    {
      "id": 17,
      "left_edge": 51,
      "top_edge": 69,
      "right_edge": 92,
      "bottom_edge": 91,
      "position": "right",
      "width_category": "medium",
      "y_group_id": 6,
      "confidence_avg": 0.91,
      "char_count": 769,
      "word_count": 117,
      "texts": [
        "minimize",
        "(2)",
        "We find these relaxations are particularly desirable for S0-",
        "cial representation learning: First, the order independence",
        "assumption better captures a sense of nearness' that is pro-",
        "vided by random walks. Moreover , this relaxation is quite",
        "useful for speeding up the training time by building small",
        "models as one vertex is given at a time",
        "Solving the optimization problem from Eq: 2 builds repre-",
        "sentations that capture the shared similarities in local graph",
        "structure between vertices. Vertices which have similar neigh-",
        "borhoods will acquire similar representations (encoding CO-",
        "citation similarity) and allowing generalization on machine )",
        "learning tasks.",
        "By combining both truncated random walks and neural",
        "language models we formulate a method which satisfies all"
      ]
    },
    {
      "id": 18,
      "left_edge": 83,
      "top_edge": 69,
      "right_edge": 93,
      "bottom_edge": 71,
      "position": "right",
      "width_category": "narrow",
      "y_group_id": 6,
      "confidence_avg": 0.67,
      "char_count": 13,
      "word_count": 3,
      "texts": [
        "Vi+w} | d(i))"
      ]
    },
    {
      "id": 19,
      "left_edge": 59,
      "top_edge": 48,
      "right_edge": 64,
      "bottom_edge": 50,
      "position": "center",
      "width_category": "narrow",
      "y_group_id": 5,
      "confidence_avg": 0.93,
      "char_count": 6,
      "word_count": 2,
      "texts": [
        "Pr (vi"
      ]
    },
    {
      "id": 20,
      "left_edge": 77,
      "top_edge": 48,
      "right_edge": 84,
      "bottom_edge": 50,
      "position": "right",
      "width_category": "narrow",
      "y_group_id": 5,
      "confidence_avg": 0.75,
      "char_count": 10,
      "word_count": 2,
      "texts": [
        "d(vi-1)) )"
      ]
    },
    {
      "id": 21,
      "left_edge": 61,
      "top_edge": 69,
      "right_edge": 71,
      "bottom_edge": 71,
      "position": "center",
      "width_category": "narrow",
      "y_group_id": 6,
      "confidence_avg": 0.99,
      "char_count": 15,
      "word_count": 4,
      "texts": [
        "log Pr ( {Vi-w,"
      ]
    },
    {
      "id": 22,
      "left_edge": 73,
      "top_edge": 70,
      "right_edge": 81,
      "bottom_edge": 71,
      "position": "right",
      "width_category": "narrow",
      "y_group_id": 6,
      "confidence_avg": 0.47,
      "char_count": 11,
      "word_count": 2,
      "texts": [
        "Vi-1, Vi+l,"
      ]
    }
  ]
}