{
  "page_number": 4,
  "total_buckets": 11,
  "last_bucket_previous_page": {
    "id": 22,
    "left_edge": 73,
    "top_edge": 70,
    "right_edge": 81,
    "bottom_edge": 71,
    "position": "right",
    "width_category": "narrow",
    "y_group_id": 6,
    "confidence_avg": 0.47,
    "char_count": 11,
    "word_count": 2,
    "texts": [
      "Vi-1, Vi+l,"
    ]
  },
  "buckets": [
    {
      "id": 0,
      "left_edge": 9,
      "top_edge": 7,
      "right_edge": 45,
      "bottom_edge": 28,
      "position": "left",
      "width_category": "narrow",
      "y_group_id": 0,
      "confidence_avg": 0.84,
      "char_count": 291,
      "word_count": 68,
      "texts": [
        "Algorithm 1 DEEP WALK(G , d, Y t) U ,",
        "Input: graph G(V; E)",
        "window size W",
        "embedding size d",
        "walks per vertex",
        "walk length t",
        "Output: matrix of vertex representations $ € RlVIxd",
        "l: Initialization: Sample $ from ulvIxd",
        "2: Build a binary Tree T from V",
        "3: for % 0 to > do",
        "4:",
        "5: for each € do Ui",
        "6=",
        "7:",
        "8=",
        "9: end for"
      ]
    },
    {
      "id": 1,
      "left_edge": 9,
      "top_edge": 31,
      "right_edge": 48,
      "bottom_edge": 59,
      "position": "left",
      "width_category": "narrow",
      "y_group_id": 2,
      "confidence_avg": 0.87,
      "char_count": 888,
      "word_count": 148,
      "texts": [
        "of our desired properties. This method generates represen-",
        "tations of social networks that are low-dimensional; and @X-",
        "ist in a continuous vector space. Its representations encode",
        "latent   forms of community membership, and because the",
        "method outputs useful intermediate representations, it can",
        "adapt to changing network topology:",
        "4 METHOD",
        "In this section we discuss the main   components of our",
        "algorithm: We also present several variants of our approach",
        "and discuss their merits.",
        "4.1 Overview",
        "As in any language modeling algorithm; the only required",
        "input is a corpus and a vocabulary V. DEEPWALK considers",
        "a set of short truncated random walks its own corpus, and",
        "the graph vertices as its own vocabulary",
        "is beneficial to know the V and the frequency distribution",
        "of vertices in the random walks ahead of the training; it is",
        "not necessary for the algorithm to work as we will show in",
        "4.2.2.",
        "V). While it"
      ]
    },
    {
      "id": 2,
      "left_edge": 8,
      "top_edge": 60,
      "right_edge": 48,
      "bottom_edge": 91,
      "position": "left",
      "width_category": "medium",
      "y_group_id": 3,
      "confidence_avg": 0.9,
      "char_count": 1218,
      "word_count": 220,
      "texts": [
        "4.2 Algorithm: DEEP WALK",
        "The algorithm consists of two main components; first a",
        "random walk generator and second an update procedure.",
        "The random walk generator takes a graph G and samples",
        "uniformly random a vertex Ui as the root of the random",
        "walk Wvi A walk   samples  uniformly from the neighbors",
        "of the last vertex visited until the maximum length (t) is",
        "reached. While we set the length of our random walks in",
        "the experiments to be fixed, there is no restriction for the",
        "random walks to be of the same length: These walks could",
        "have restarts (i.e. a teleport probability of returning back",
        "to their root), but our preliminary results did not show any",
        "advantage of using restarts: In practice, our implementation",
        "specifies a number of random walks Y of length t to start at",
        "each vertex.",
        "Lines 3-9 in Algorithm 1 shows the core of our approach:",
        "The outer loop specifies the number of times, Y, which wC",
        "should start random walks at each vertex We think of each",
        "iteration making as a pass over the data and sample one",
        "walk per node during this pass. At the start of each pass we",
        "generate a random ordering to traverse the vertices. This",
        "is not strictly required, but is well-known to speed up the",
        "convergence of stochastic gradient descent_"
      ]
    },
    {
      "id": 3,
      "left_edge": 52,
      "top_edge": 7,
      "right_edge": 80,
      "bottom_edge": 16,
      "position": "center",
      "width_category": "narrow",
      "y_group_id": 0,
      "confidence_avg": 0.95,
      "char_count": 155,
      "word_count": 47,
      "texts": [
        "Algorithm 2 SkipGram(d , Wvi' w)",
        "1: for each Vj € Wvi do",
        "2: for each Uk € Wvi [j ~ w : j + w] do",
        "3:",
        "4:",
        "5: end for",
        "6: end for",
        "J(d) = _ log Pr(uk | d(vj))",
        "$ = $ -a *"
      ]
    },
    {
      "id": 4,
      "left_edge": 51,
      "top_edge": 19,
      "right_edge": 91,
      "bottom_edge": 64,
      "position": "right",
      "width_category": "medium",
      "y_group_id": 1,
      "confidence_avg": 0.95,
      "char_count": 1656,
      "word_count": 290,
      "texts": [
        "In the inner loop, we iterate over all the vertices of the",
        "graph. For each vertex Vi we generate a random walk IWvi",
        "t, and then use it to update our representations (Line 7). We",
        "use the SkipGram algorithm [26] to update these represen-",
        "tations in accordance with our objective function in Eq: 2_",
        "4.2.1 SkipGram",
        "SkipGram is a language model that maximizes the CO-",
        "occurrence probability among the words that appear within",
        "a window, W, in a sentence [26].",
        "Algorithm 2 iterates over all possible collocations in ran-",
        "dom walk that appear within the window w (lines 1-2) . For",
        "each; we map each vertex Uj to its current representation",
        "vector @(uj) € Rd (See Figure 3b). Given the representa-",
        "tion of Uj , we would like to maximize the probability of its",
        "neighbors in the walk (line 3). We can learn such posterior",
        "distribution using several choices of classifiers. For exam-",
        "ple, modeling the previous problem using logistic regression",
        "would result in a huge number of labels that is equal to |V|",
        "which could be in millions or billions. Such models require",
        "large amount of computational resources that could span a",
        "whole cluster of computers [3]. To speed the training time;",
        "Hierarchical Softmax [29,30] can be used to approximate the",
        "probability distribution.",
        "4.2.2 Hierarchical Softmax",
        "Given that Uk € V, calculating Pr(uk $(v;)) in line 3",
        "is not feasible: Computing the partition function (normal-",
        "ization factor) is expensive. If we assign the vertices to the",
        "leaves of a binary tree, the prediction problem turns into",
        "maximizing the probability of a specific path in the tree",
        "(See Figure 3c). If the path to vertex Uk is identified by",
        "a sequence of tree nodes  (b0, b1,-",
        "Uk: ) then",
        "root,"
      ]
    },
    {
      "id": 5,
      "left_edge": 51,
      "top_edge": 63,
      "right_edge": 57,
      "bottom_edge": 65,
      "position": "center",
      "width_category": "narrow",
      "y_group_id": 3,
      "confidence_avg": 0.61,
      "char_count": 10,
      "word_count": 2,
      "texts": [
        "bflog IVII"
      ]
    },
    {
      "id": 6,
      "left_edge": 52,
      "top_edge": 66,
      "right_edge": 91,
      "bottom_edge": 91,
      "position": "right",
      "width_category": "narrow",
      "y_group_id": 4,
      "confidence_avg": 0.85,
      "char_count": 787,
      "word_count": 134,
      "texts": [
        "Pr(uk d(v;)) II Pr(b1 d(v;))",
        "1=1",
        "Now, Pr(b1 d(v;)) could be modeled by a binary classifier",
        "that is assigned to the parent of the node b1. This reduces",
        "the computational complexity of calculating Pr(uk $(v;))",
        "from O(IVI) to O(log|VV):",
        "We can speed up the training process further, by assigning",
        "shorter paths to the frequent vertices in the random walks.",
        "Huffman coding is used to reduce the access time of frequent",
        "elements in the tree.",
        "4.2.3 Optimization",
        "The model parameter set is {d,T} where the size of each",
        "is O(dlvI): Stochastic gradient descent (SGD) [4] is used",
        "to optimize these parameters (Line 4, Algorithm 2)- The",
        "derivatives are estimated using the back-propagation algo",
        "rithm. The learning rate Q for SGD is initially set to 2.5%",
        "at the beginning of the training and then decreased linearly"
      ]
    },
    {
      "id": 7,
      "left_edge": 16,
      "top_edge": 21,
      "right_edge": 23,
      "bottom_edge": 22,
      "position": "left",
      "width_category": "narrow",
      "y_group_id": 1,
      "confidence_avg": 0.53,
      "char_count": 11,
      "word_count": 2,
      "texts": [
        "Shuffle(V )"
      ]
    },
    {
      "id": 8,
      "left_edge": 13,
      "top_edge": 23,
      "right_edge": 34,
      "bottom_edge": 27,
      "position": "left",
      "width_category": "narrow",
      "y_group_id": 1,
      "confidence_avg": 1.0,
      "char_count": 53,
      "word_count": 12,
      "texts": [
        "Wvi RandomW alk(G , Vi,t)",
        "SkipGram( d , Wvi' w)",
        "end for"
      ]
    },
    {
      "id": 9,
      "left_edge": 76,
      "top_edge": 62,
      "right_edge": 85,
      "bottom_edge": 63,
      "position": "right",
      "width_category": "narrow",
      "y_group_id": 3,
      "confidence_avg": 0.96,
      "char_count": 21,
      "word_count": 6,
      "texts": [
        "2 b[log _ IvI1) , (bo"
      ]
    },
    {
      "id": 10,
      "left_edge": 70,
      "top_edge": 65,
      "right_edge": 75,
      "bottom_edge": 66,
      "position": "right",
      "width_category": "narrow",
      "y_group_id": 3,
      "confidence_avg": 0.65,
      "char_count": 9,
      "word_count": 2,
      "texts": [
        "[log IVII"
      ]
    }
  ]
}