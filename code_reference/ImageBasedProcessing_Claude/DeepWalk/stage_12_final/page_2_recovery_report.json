{
  "page_number": "2",
  "analysis": {
    "total_buckets": 4,
    "filtered_buckets": [],
    "matched_buckets": [
      {
        "bucket_id": 0,
        "text": "narios, we   evaluate its performance on challenging  multi- label network classification problems in large heterogeneous graphs. In the relational classification problem, the links be- tween feature vectors violate the traditional i..d. assump- tion. Techniques to address this problem typically use ap- proximate inference techniques [31, 35] to leverage the de- pendency information to improve classification results. We distance ourselves from these approaches by learning label- independent representations of the graph: Our representa- tion quality is not influenced by the choice of labeled ver _ tices, s0 they can be shared among tasks: DEEP WALK outperforms other latent representation meth- ods for creating social dimensions [39,41], especially when labeled nodes are scarce. Strong performance with our rep- resentations is  possible with very  simple linear classifiers (e.g: logistic regression). Our representations are general, and can be   combined with any  classification method (in- cluding iterative inference methods) DEEPWALK achieves all of that while being an online algorithm that is trivially parallelizable. Our contributions are as follows: We introduce deep learning as a tool to analyze graphs, to build robust representations that are suitable for statistical modeling: DEEPWALK learns structural reg- ularities present within short random walks: We extensively evaluate our representations on multi- label classification tasks on several social networks. We show significantly increased classification performance in the presence of label sparsity, getting improvements 5%-10% of Micro F1, on the sparsest problems we con- cider Tn outperform its competitors even when given 60% less training data cacoc DEFPWAI K'c renrecentations can",
        "confidence": 0.9808541973490427,
        "method": "fuzzy_match"
      },
      {
        "bucket_id": 1,
        "text": "We demonstrate the scalability of our algorithm by building representations of web-scale graphs, (such as YouTube) using a parallel implementation: Moreover, we describe the minimal changes necessary to build a streaming version of our approach: The rest of the paper is arranged as follows. In Sections 2 and 3, we discuss the problem formulation of classification in data networks, and how it relates to our work: In Section 4 we   present DEEPWALK, our approach  for Social Repre- sentation Learning: We outline ours experiments in Section 5, and present their results in Section 6. We close with a discussion of related work in Section 7, and our conclusions.",
        "confidence": 0.9968895800933126,
        "method": "fuzzy_match"
      },
      {
        "bucket_id": 2,
        "text": "2. PROBLEM DEFINITION We consider the problem of classifying members of a social network into one or more categories. More formally, let G (V,E), where V are the members of the network; and E be its edges, E = (V x V): Given partially labeled social a (V,E,X,Y), with attributes X € RlVIx s where S is the size of the feature space for each attribute vector; and Y € RlvIxl V is the set of labels: In a traditional machine learning classification setting; we aim to learn a hypothesis H that maps elements of X to the labels set V. In our case, we can utilize the significant in- formation about the dependence of the examples embedded in the structure of G to achieve superior performance. network GL",
        "confidence": 0.973293768545994,
        "method": "fuzzy_match"
      },
      {
        "bucket_id": 3,
        "text": "In the literature, this is known as the relational classifi- cation (or the collective classification problem [37]). Tradi- tional approaches to relational classification pose the prob- lem an inference in as an undirected Markov network; and then use iterative approximate inference algorithms (such as the iterative   classification algorithm [31], Gibbs Sam- pling [14], or label relaxation [18]) to compute the posterior distribution of labels given the network structure We propose a different approach to capture the network topology information. Instead of mixing the label space as part of the feature space, we propose an unsupervised method which learns features that capture the graph struc- ture independent of the labels' distribution. This separation between the structural representation and the labeling task avoids cascading errors; which can occur in iterative methods [33]. Moreover, the same representation can be used for multiple classification problems concerning that network Our goal is to learn XE RlvIxd 6 where d is small num- ber of latent dimensions. These low-dimensional represen- tations are distributed; meaning each social phenomena is expressed by a subset of the dimensions and each dimension contributes to a subset of the social concepts expressed by the space. Using these structural features; we will augment the at- tributes space to help the classification decision. These fea- tures are general, and can be used with any classification algorithm (including iterative methods). However , we be- lieve that the greatest utility of these features is their easy integration with simple machine learning algorithms: They scale appropriately in real-world networks, as we will show in Section 6. 3. LEARNING SOCIAL REPRESENTATIONS We seek learning social representations with the following characteristics: Adaptability Real social networks are constantly evolving; new  social relations  should not require re- peating the learning process all over again. Community aware The distance between latent dimensions   should represent metric a for evaluating social similarity between the corresponding members of the network_ This allows generalization in networks with homophily: Low dimensional When labeled data is scarce, low- dimensional models   generalize better, and speed up convergence and inference. Continuous We require latent  representations to model partial community membership in continuous space In addition to providing nuanced a view of community membership, a continuous representation has smooth decision boundaries between communities which allows more robust classification. Our method for satisfying these requirements learns repre sentation for vertices from a stream of short random walks, using optimization  techniques originally designed for lan- guage modeling: Here, we review the basics of both random walks and language modeling; and describe how their com- bination satisfies our requirements.",
        "confidence": 0.9928782351919403,
        "method": "fuzzy_match"
      }
    ],
    "missing_buckets": [],
    "llm_reported_missing": []
  },
  "recovery": {
    "buckets_recovered": [],
    "buckets_gibberish": [],
    "recovery_notes": "No missing buckets"
  }
}