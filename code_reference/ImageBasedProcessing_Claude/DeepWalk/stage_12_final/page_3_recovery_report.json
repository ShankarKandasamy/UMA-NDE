{
  "page_number": "3",
  "analysis": {
    "total_buckets": 23,
    "filtered_buckets": [],
    "matched_buckets": [
      {
        "bucket_id": 0,
        "text": "Frequency of Vertex Occurrence_in Short Random Walks 10",
        "confidence": 1.0,
        "method": "exact_match"
      },
      {
        "bucket_id": 1,
        "text": "103 \"5 102 # 101 109 109",
        "confidence": 1.0,
        "method": "exact_match"
      },
      {
        "bucket_id": 2,
        "text": "101",
        "confidence": 1.0,
        "method": "exact_match"
      },
      {
        "bucket_id": 3,
        "text": "Vertex visitation count 102 103 10\" (a) YouTube Social Graph",
        "confidence": 1.0,
        "method": "exact_match"
      },
      {
        "bucket_id": 4,
        "text": "105",
        "confidence": 1.0,
        "method": "exact_match"
      },
      {
        "bucket_id": 5,
        "text": "106",
        "confidence": 1.0,
        "method": "exact_match"
      },
      {
        "bucket_id": 6,
        "text": "106 Frequency of Word Occurrence in Wikipedia 105 10' 1 103 # 102 101 109 109 101 102 103 10' 105 106 107 Word mention count (b) Wikipedia Article Text",
        "confidence": 1.0,
        "method": "exact_match"
      },
      {
        "bucket_id": 7,
        "text": "Figure 2: The power-law distribution of vertices appearing in short random walks (2a) follows power-law, much like a the distribution of words in natural language (2b).",
        "confidence": 0.9875776397515527,
        "method": "fuzzy_match"
      },
      {
        "bucket_id": 8,
        "text": "3.1 Random Walks We denote a random walk rooted at vertex Ui as Wvi  It is a stochastic process with random variables WI w2 Ui ) Ui > Wk Ui such that Wk+1 is a vertex chosen at random from the neigh- Ui bors of vertex Uk - Random walks have been used as a sim- ilarity measure for a variety of problems in content recom- mendation [11] and community detection [1] They are also the foundation of a class of output sensitive algorithms which use them to compute local community structure information in time sublinear to the size of the input graph [38]. It is this connection to local structure that motivates us to use a stream of short random walks as our basic tool for ex- tracting information from a network. In addition to captur- ing community information, using random walks as the ba- sis for our algorithm gives us two other desirable properties. First, local exploration is easy to parallelize: Several random walkers (in different threads, processes, or machines) can si- multaneously explore dlferent parts Of the same graph. ondly, relying on information obtained from short random walks make it possible to accommodate small changes in the graph structure without the need for global recomputation. We can iteratively update the learned model with new ran- dom walks from the changed region in time sub-linear to the entire graph. 3.2 Connection: Power laws Having chosen online random walks as our primitive for capturing graph structure, we now need suitable method a to capture this information: If the degree distribution of a connected graph follows a power law (is   scale-free) , we observe that the frequency which vertices appear in the short random walks will also follow power-law distribution. a Word frequency in natural language follows a similar dis- tribution, and techniques from language modeling account for this distributional behavior. To emphasize this similar- ity we show two different power-law distributions in Figure 2_ The first comes from a series of short random walks on a scale-free graph; and the second comes from the text of 100,000 articles from the English Wikipedia. A core contribution of our work is the idea that techniques which have been used to model natural language (where the symbol frequency follows a power law distribution (or Zipf '$ law) ) can be re-purposed to model community structure in networks. We spend the rest of this section reviewing the growing work in language modeling, and transforming it to learn representations of vertices which satisfy OUr criteria. 3.3 Language Modeling",
        "confidence": 0.98757016840417,
        "method": "fuzzy_match"
      },
      {
        "bucket_id": 9,
        "text": "The goal of language modeling is estimate the likelihood of a specific sequence of words appearing in a corpus. More formally; given a sequence of words",
        "confidence": 0.98,
        "method": "fuzzy_match"
      },
      {
        "bucket_id": 10,
        "text": "where Wi € V (V is the vocabulary) , we would like to maxi- mize the Pr(wn|wo, U1 , pus. Recent work in representation learning has focused on us- ing probabilistic neural networks to build general representa- tions of words which extend the scope of language modeling beyond its original goals. In this work, we present a generalization of language mod- eling to explore the graph through a stream of short random walks. These walks can be thought of short sentences and phrases in a special language. The direct analog is to @S- timate the likelihood of observing   vertex Ui given all the previous vertices visited s0 far in the random walk. Wn-1) over all the training COr-",
        "confidence": 0.9279141104294478,
        "method": "fuzzy_match"
      },
      {
        "bucket_id": 15,
        "text": "Our goal is to learn a latent representation; not only a probability distribution of node co-occurrences_ and SO we introduce a mapping function $: v € V + Rlvixa This mapping $ represents the latent social representation aSSo- ciated with each vertex U in the graph. (In practice, wC represent $ by a |Vl X d matrix of free parameters; which will serve later on as our XE.) The problem then; is to estimate the likelihood:",
        "confidence": 0.92,
        "method": "fuzzy_match"
      },
      {
        "bucket_id": 16,
        "text": "(1) However as the walk length grows, computing this objec- tive function becomes unfeasible. A recent relaxation in language  modeling [26, 27] turns the prediction problem on its head. First, instead of using the context to predict missing word, it a uses one word to predict the context. Secondly; the context is composed of the words appearing to right side of the given word as well as the left side. Finally; it removes the ordering constraint on the problem: Instead, the model is required to maximize the probability of any word appearing in the context without the knowledge of its offset from the given word: In terms of vertex representation modeling; this yields the optimization problem:",
        "confidence": 0.9911634756995582,
        "method": "fuzzy_match"
      },
      {
        "bucket_id": 17,
        "text": "minimize (2) We find these relaxations are particularly desirable for S0- cial representation learning: First, the order independence assumption better captures a sense of nearness' that is pro- vided by random walks. Moreover , this relaxation is quite useful for speeding up the training time by building small models as one vertex is given at a time Solving the optimization problem from Eq: 2 builds repre- sentations that capture the shared similarities in local graph structure between vertices. Vertices which have similar neigh- borhoods will acquire similar representations (encoding CO- citation similarity) and allowing generalization on machine ) learning tasks. By combining both truncated random walks and neural language models we formulate a method which satisfies all",
        "confidence": 0.9874255459960292,
        "method": "fuzzy_match"
      },
      {
        "bucket_id": 18,
        "text": "Vi+w} | d(i))",
        "confidence": 0.75,
        "method": "word_coverage"
      },
      {
        "bucket_id": 21,
        "text": "log Pr ( {Vi-w,",
        "confidence": 0.75,
        "method": "word_coverage"
      }
    ],
    "missing_buckets": [
      {
        "bucket_id": 11,
        "text": "WI n = (wo, U1 ,",
        "texts": [
          "WI n = (wo, U1 ,"
        ],
        "position": "right",
        "width_category": "narrow",
        "y_group_id": 0,
        "confidence_avg": 1.0,
        "char_count": 16,
        "word_count": 6,
        "confidence_missing": 0.30000000000000004,
        "method_tried": "missing"
      },
      {
        "bucket_id": 12,
        "text": "Wn )",
        "texts": [
          "Wn )"
        ],
        "position": "right",
        "width_category": "narrow",
        "y_group_id": 0,
        "confidence_avg": 0.53,
        "char_count": 4,
        "word_count": 2,
        "confidence_missing": 0.5,
        "method_tried": "missing"
      },
      {
        "bucket_id": 13,
        "text": "Pr (Ui (01, U2 ,",
        "texts": [
          "Pr (Ui (01, U2 ,"
        ],
        "position": "right",
        "width_category": "narrow",
        "y_group_id": 4,
        "confidence_avg": 0.74,
        "char_count": 16,
        "word_count": 5,
        "confidence_missing": 0.4545454545454546,
        "method_tried": "missing"
      },
      {
        "bucket_id": 14,
        "text": ", Vi-1))",
        "texts": [
          ", Vi-1))"
        ],
        "position": "right",
        "width_category": "narrow",
        "y_group_id": 4,
        "confidence_avg": 0.82,
        "char_count": 8,
        "word_count": 2,
        "confidence_missing": 0.25,
        "method_tried": "missing"
      },
      {
        "bucket_id": 19,
        "text": "Pr (vi",
        "texts": [
          "Pr (vi"
        ],
        "position": "center",
        "width_category": "narrow",
        "y_group_id": 5,
        "confidence_avg": 0.93,
        "char_count": 6,
        "word_count": 2,
        "confidence_missing": 0.19999999999999996,
        "method_tried": "missing"
      },
      {
        "bucket_id": 20,
        "text": "d(vi-1)) )",
        "texts": [
          "d(vi-1)) )"
        ],
        "position": "right",
        "width_category": "narrow",
        "y_group_id": 5,
        "confidence_avg": 0.75,
        "char_count": 10,
        "word_count": 2,
        "confidence_missing": 0.16666666666666652,
        "method_tried": "missing"
      },
      {
        "bucket_id": 22,
        "text": "Vi-1, Vi+l,",
        "texts": [
          "Vi-1, Vi+l,"
        ],
        "position": "right",
        "width_category": "narrow",
        "y_group_id": 6,
        "confidence_avg": 0.47,
        "char_count": 11,
        "word_count": 2,
        "confidence_missing": 0.2222222222222221,
        "method_tried": "missing"
      }
    ],
    "llm_reported_missing": [
      0,
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8,
      9,
      10,
      11,
      12,
      13,
      14,
      15,
      16,
      17,
      18,
      19,
      20,
      21,
      22
    ]
  },
  "recovery": {
    "markdown": "# Frequency of Vertex Occurrence in Short Random Walks\n10\n\n## 103\n\"5 102 #\n101\n109\n109\n\n### 101\n\n### Vertex visitation count\n102 103 10\"\n(a) YouTube Social Graph\n\n### 105\n\n### 106\n\n#### Frequency of Word Occurrence in Wikipedia\n105\n10'\n1 103\n# 102\n101\n109\n109 101 102 103 10' 105 106 107\nWord mention count\n(b) Wikipedia Article Text\n\n## Figure 2: The power-law distribution of vertices appearing in short random walks (2a) follows power-law, much like the distribution of words in natural language (2b).\n\n### 3.1 Random Walks\nWe denote a random walk rooted at vertex \\( U_i \\) as \\( W_{v_i} \\). It is a stochastic process with random variables \\( W_1, W_2, \\ldots, W_k \\) such that \\( W_k \\) is a vertex chosen at random from the neighbors of vertex \\( U_k \\). Random walks have been used as a similarity measure for a variety of problems in content recommendation [11] and community detection [1]. They are also the foundation of a class of output sensitive algorithms which use them to compute local community structure information in time sublinear to the size of the input graph [38]. It is this connection to local structure that motivates us to use a stream of short random walks as our basic tool for extracting information from a network. In addition to capturing community information, using random walks as the basis for our algorithm gives us two other desirable properties. First, local exploration is easy to parallelize: Several random walkers (in different threads, processes, or machines) can simultaneously explore different parts of the same graph. Secondly, relying on information obtained from short random walks makes it possible to accommodate small changes in the graph structure without the need for global recomputation. We can iteratively update the learned model with new random walks from the changed region in time sub-linear to the entire graph.\n\n### 3.2 Connection: Power laws\nHaving chosen online random walks as our primitive for capturing graph structure, we now need a suitable method to capture this information: If the degree distribution of a connected graph follows a power law (is scale-free), we observe that the frequency with which vertices appear in the short random walks will also follow a power-law distribution. Word frequency in natural language follows a similar distribution, and techniques from language modeling account for this distributional behavior. To emphasize this similarity we show two different power-law distributions in Figure 2. The first comes from a series of short random walks on a scale-free graph, and the second comes from the text of 100,000 articles from the English Wikipedia. A core contribution of our work is the idea that techniques which have been used to model natural language (where the symbol frequency follows a power law distribution (or Zipf's law)) can be repurposed to model community structure in networks. We spend the rest of this section reviewing the growing work in language modeling, and transforming it to learn representations of vertices which satisfy our criteria.\n\n### 3.3 Language Modeling\nThe goal of language modeling is to estimate the likelihood of a specific sequence of words appearing in a corpus. More formally, given a sequence of words\n\n$$ W_n = (w_0, w_1, \\ldots, w_n) $$\nwhere \\( w_i \\in V \\) (V is the vocabulary), we would like to maximize the \\( Pr(w_n | w_0, U_1, \\ldots, w_{n-1}) \\) over all the training corpus.\n\nRecent work in representation learning has focused on using probabilistic neural networks to build general representations of words which extend the scope of language modeling beyond its original goals. In this work, we present a generalization of language modeling to explore the graph through a stream of short random walks. These walks can be thought of as short sentences and phrases in a special language. The direct analog is to estimate the likelihood of observing vertex \\( U_i \\) given all the previous vertices visited so far in the random walk.\n\n$$ Pr(w_n | (v_1, v_2, \\ldots, v_{n-1})) $$\nOur goal is to learn a latent representation, not only a probability distribution of node co-occurrences, and so we introduce a mapping function \\( \\phi: V \\to \\mathbb{R}^{l \\times d} \\). This mapping \\( \\phi \\) represents the latent social representation associated with each vertex \\( U \\) in the graph. (In practice, we represent \\( \\phi \\) by a \\( |V| \\times d \\) matrix of free parameters, which will serve later on as our \\( XE \\).) The problem then, is to estimate the likelihood:\n\n$$ \\text{(1)} $$\nHowever, as the walk length grows, computing this objective function becomes unfeasible. A recent relaxation in language modeling [26, 27] turns the prediction problem on its head. First, instead of using the context to predict a missing word, it uses one word to predict the context. Secondly, the context is composed of the words appearing to the right side of the given word as well as the left side. Finally, it removes the ordering constraint on the problem: Instead, the model is required to maximize the probability of any word appearing in the context without the knowledge of its offset from the given word:\n\nIn terms of vertex representation modeling, this yields the optimization problem:\n\n$$ \\text{minimize } \\phi $$\n$$ - \\log Pr(t_i | (v_{i-1}, v_{i+1}, \\ldots, v_{i+w})) | \\phi(v_i) $$\nWe find these relaxations are particularly desirable for social representation learning: First, the order independence assumption better captures a sense of 'nearness' that is provided by random walks. Moreover, this relaxation is quite useful for speeding up the training time by building small models as one vertex is given at a time. Solving the optimization problem from Eq. (2) builds representations that capture the shared similarities in local graph structure between vertices. Vertices which have similar neighborhoods will acquire similar representations (encoding co-citation similarity), allowing generalization on machine learning tasks. By combining both truncated random walks and neural language models, we formulate a method which satisfies all.",
    "buckets_recovered": [
      11,
      12,
      13,
      14,
      19,
      20,
      22
    ],
    "buckets_gibberish": [],
    "recovery_notes": "Inserted all missing buckets at their appropriate locations based on context and reading order."
  }
}